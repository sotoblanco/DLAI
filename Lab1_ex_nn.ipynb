{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building your first neural network in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imgage](img1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Youâ€™re hire as a Junior Data Scientist to work on a subscription streaming service. It is your first day in the job, you are setting up your stuff at your new desk, and suddenly, Sarah from Marketing rushes over. \"Our customer churn is through the roof! We need to know who's about to leave so we can offer them incentives.\" Your task? Predict if a customer will churn (binary target: 0 for stay, 1 for churn) based on their average monthly data usage. This is a classic business problem, and your model could save the company millions!\n",
    "\n",
    "Start by getting the data ready, in real world you are likely to get data from a database, and will need to ask for permissions to the devops teams and also the data engineering team will provide you the data you need after open some tickets, then you will need to preprocess it before feeding it into the model.\n",
    "\n",
    "For now, let's skip the data engineering part and just use a small dataset of 10 customers and convert into tensors to be able to use it in our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of average monthly data usage (in GB) for 10 customers\n",
    "avg_month_usage = [\n",
    "    5.2,  # Customer 1: Low usage, might churn\n",
    "    25.8, # Customer 2: High usage, likely to stay\n",
    "    1.5,  # Customer 3: Very low usage, high churn risk\n",
    "    18.7, # Customer 4: Moderate usage\n",
    "    10.1, # Customer 5: Low-ish usage\n",
    "    30.5, # Customer 6: Very high usage, loyal\n",
    "    7.3,  # Customer 7: Low usage\n",
    "    22.0, # Customer 8: Good usage\n",
    "    3.9,  # Customer 9: Low usage, potential churn\n",
    "    15.0  # Customer 10: Moderate usage\n",
    "]\n",
    "\n",
    "# List of churn status (0 = No Churn, 1 = Churn) for the same 10 customers\n",
    "churn = [\n",
    "    1,    # Customer 1: Churned\n",
    "    0,    # Customer 2: Did not churn\n",
    "    1,    # Customer 3: Churned\n",
    "    0,    # Customer 4: Did not churn\n",
    "    1,    # Customer 5: Churned\n",
    "    0,    # Customer 6: Did not churn\n",
    "    1,    # Customer 7: Churned\n",
    "    0,    # Customer 8: Did not churn\n",
    "    1,    # Customer 9: Churned\n",
    "    0     # Customer 10: Did not churn\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_month_usage_tensor = torch.tensor(avg_month_usage).float().reshape(-1, 1)\n",
    "churn_tensor = torch.tensor(churn).float()\n",
    "\n",
    "print(f\"avg_month_usage_tensor:\\n{avg_month_usage_tensor}\\n\\nchurn_tensor:\\n{churn_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code to create a ``TensorDataset`` and ``DataLoader`` for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4 # Adjusted batch size for smaller dataset\n",
    "num_samples = len(avg_month_usage_tensor)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "customer_dataset = TensorDataset(avg_month_usage_tensor, churn_tensor)\n",
    "customer_dataloader = DataLoader(customer_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(f\"Dummy DataLoader created with {len(customer_dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Define the Model (Single Layer) ---\n",
    "# For a binary classification, the output layer typically has 1 neuron.\n",
    "# We'll use a simple linear layer.\n",
    "# The input_features should match the number of features in your dataset.\n",
    "input_features = 1 \n",
    "output_features = 1 # For binary classification\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple single-layer neural network for binary classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1) # Single linear layer for output\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor (logits).\n",
    "        \"\"\"\n",
    "        return self.linear(x)\n",
    "\n",
    "model = BinaryClassifier(input_features)\n",
    "print(\"Model defined:\", model)\n",
    "\n",
    "# --- 2. Define the Loss Function ---\n",
    "# BCEWithLogitsLoss combines a Sigmoid layer and the BCELoss in one single class.\n",
    "# This version is more numerically stable than using a plain Sigmoid followed by BCELoss.\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "print(\"Loss function defined:\", criterion)\n",
    "\n",
    "# --- 3. Define the Optimizer ---\n",
    "# Stochastic Gradient Descent (SGD)\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "print(\"Optimizer defined:\", optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_binary_classifier(model, dataloader, criterion, optimizer, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Performs the training loop for a binary classification model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model to be trained.\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader providing training data.\n",
    "        criterion (torch.nn.Module): The loss function (e.g., nn.BCEWithLogitsLoss).\n",
    "        optimizer (torch.optim.Optimizer): The optimizer (e.g., optim.SGD).\n",
    "        num_epochs (int): The number of epochs to train the model.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in None:\n",
    "            # Zero the parameter gradients\n",
    "\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = None\n",
    "\n",
    "            # Calculate loss\n",
    "            # Ensure labels are float and have the correct shape (e.g., [batch_size, 1])\n",
    "            loss = None\n",
    "\n",
    "            # Backward pass and optimize\n",
    "\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittests import test_train_binary_classifier\n",
    "test_train_binary_classifier(train_binary_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Run the Training Loop ---\n",
    "print(\"\\nStarting training...\")\n",
    "train_binary_classifier(model, customer_dataloader, criterion, optimizer, num_epochs=10) # Increased epochs for small dataset\n",
    "print(\"Training finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
